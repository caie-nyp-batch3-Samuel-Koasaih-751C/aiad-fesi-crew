# AIAD FESI Crew Project

This project implements a full machine learning workflow using **Kedro**, **Docker**, and **Kubernetes** with a Flask UI for inference.  
It covers data ingestion, preprocessing, masking, dataset splitting, model training, and a production-ready UI served via Gunicorn behind Kubernetes with autoscaling.

---

## Project Structure

aiad-fesi-crew/
â”œâ”€â”€ conf/ # Kedro configs (catalog, parameters, etc.)
â”œâ”€â”€ data/ # Local data (ignored in Docker/K8s)
â”œâ”€â”€ docker/ # Dockerfiles and entrypoints
â”‚ â”œâ”€â”€ Dockerfile.kedro
â”‚ â”œâ”€â”€ Dockerfile.train
â”‚ â”œâ”€â”€ Dockerfile.ui
â”‚ â”œâ”€â”€ entrypoint.sh
â”‚ â”œâ”€â”€ entrypoint.train.sh
â”‚ â”œâ”€â”€ gunicorn.conf.py
â”‚ â””â”€â”€ requirements.ui.txt
â”œâ”€â”€ k8s/ # Kubernetes manifests
â”‚ â”œâ”€â”€ cm-kedro-conf.yaml
â”‚ â”œâ”€â”€ job-ingestion.yaml
â”‚ â”œâ”€â”€ job-data-preprocessing.yaml
â”‚ â”œâ”€â”€ job-mask-apply.yaml
â”‚ â”œâ”€â”€ job-split.yaml
â”‚ â”œâ”€â”€ job-train.yaml
â”‚ â”œâ”€â”€ pvc-data.yaml
â”‚ â”œâ”€â”€ deploy-ui.yaml
â”‚ â”œâ”€â”€ ui-service.yaml
â”‚ â”œâ”€â”€ ui-hpa.yaml
â”‚ â””â”€â”€ ui-ingress.yaml
â”œâ”€â”€ notebooks/ # Jupyter notebooks (EDA, prototyping)
â”œâ”€â”€ src/aiad_fesi_crew/ # Main Kedro + UI source
â”‚ â”œâ”€â”€ pipelines/ # Kedro pipelines
â”‚ â”‚ â”œâ”€â”€ data_ingestion/
â”‚ â”‚ â”œâ”€â”€ data_preprocessing/
â”‚ â”‚ â”œâ”€â”€ mask_merge/
â”‚ â”‚ â”œâ”€â”€ data_split/
â”‚ â”‚ â””â”€â”€ training/
â”‚ â””â”€â”€ ui/ # Flask UI
â”‚ â”œâ”€â”€ static/
â”‚ â”œâ”€â”€ templates/
â”‚ â”œâ”€â”€ init.py # create_app()
â”‚ â”œâ”€â”€ routes.py
â”‚ â””â”€â”€ inference.py
â”œâ”€â”€ docker-compose.yml # Local multi-service runner
â”œâ”€â”€ pyproject.toml # Dependencies
â”œâ”€â”€ requirements.txt # Python deps
â””â”€â”€ README.md # You are here


---

## Running Kedro Pipelines Locally

1. Install dependencies:

```bash
pip install -r requirements.txt

    Run a pipeline:

kedro run --pipeline data_split
kedro run --pipeline training

Pipelines available:

    data_ingestion

    data_preprocessing

    mask_merge

    data_split

    training

Running with Docker
Build Images

# Kedro worker image
docker build -f docker/Dockerfile.kedro -t aiad-fesi-crew-kedro:v5 .

# Training image
docker build -f docker/Dockerfile.train -t aiad-fesi-crew-train:v3 .

# UI image
docker build -f docker/Dockerfile.ui -t aiad-fesi-crew-ui:v3 .

Run UI Locally

docker run --rm -p 8000:8000 aiad-fesi-crew-ui:v3

Then open http://localhost:8000

.
Running on Kubernetes (Minikube)
Start Minikube

minikube start

Deploy Data PVC

kubectl apply -f k8s/pvc-data.yaml

Run Data/Training Jobs

kubectl apply -f k8s/job-ingestion.yaml
kubectl apply -f k8s/job-data-preprocessing.yaml
kubectl apply -f k8s/job-mask-apply.yaml
kubectl apply -f k8s/job-split.yaml
kubectl apply -f k8s/job-train.yaml

Deploy UI

kubectl apply -f k8s/deploy-ui.yaml
kubectl apply -f k8s/ui-service.yaml

(Optional) Horizontal Pod Autoscaler

kubectl apply -f k8s/ui-hpa.yaml

Ingress Access

    Enable ingress:

minikube addons enable ingress

Apply ingress:

kubectl apply -f k8s/ui-ingress.yaml

Update /etc/hosts:

192.168.49.2   ui.potatoe

(replace with your minikube ip)

Open http://ui.potatoe
âš¡ Load Testing HPA

To simulate load and test scaling:

# Install hey (Arch)
sudo pacman -S hey

# Run load test
hey -z 30s -c 50 http://ui.potatoe/

Monitor scaling:

kubectl get hpa -w

Features Implemented

    Kedro pipelines for data processing and training

    Dockerized pipelines and UI

    Kubernetes jobs for each pipeline stage

    PVC for persistent dataset sharing

    UI deployment with health probes

    Service + Ingress for external access

    Horizontal Pod Autoscaler (HPA)

    Load testing setup with hey

Demo Script (Quick Run)

# 1. Start cluster
minikube start

# 2. Deploy PVC
kubectl apply -f k8s/pvc-data.yaml

# 3. Run preprocessing + training
kubectl apply -f k8s/job-ingestion.yaml
kubectl apply -f k8s/job-data-preprocessing.yaml
kubectl apply -f k8s/job-mask-apply.yaml
kubectl apply -f k8s/job-split.yaml
kubectl apply -f k8s/job-train.yaml

# 4. Deploy UI
kubectl apply -f k8s/deploy-ui.yaml
kubectl apply -f k8s/ui-service.yaml
kubectl apply -f k8s/ui-ingress.yaml
kubectl apply -f k8s/ui-hpa.yaml

# 5. Access UI
minikube ip   # check cluster IP
# add to /etc/hosts
# <IP>   ui.potatoe

ğŸ‘¨â€ğŸ’» Authors: AIAD FESI Crew
ğŸ“… Version: August 2025